{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries regarding torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde, lognorm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30218\n",
      "30218\n",
      "(30218, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading from RSA_input.csv\n",
    "df = pd.read_csv('RSA_input.csv')\n",
    "grain_R = df[\"grain_R\"]\n",
    "grain_asp = df[\"grain_asp\"]\n",
    "print(len(grain_R)) # length 30218\n",
    "print(len(grain_asp)) # length 30218\n",
    "\n",
    "# Combined into numpy shape (30218, 2)\n",
    "grainsData = np.column_stack((grain_R, grain_asp))\n",
    "print(grainsData.shape)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data of grains (shape (30218, 2))\n",
    "# Load into DataLoader\n",
    "\n",
    "def compute_gradient_penalty(netD, real_data, fake_data):\n",
    "    batch_size = real_data.size(0)\n",
    "    device = real_data.device\n",
    "\n",
    "    # Reshape real_data and fake_data to have shape (N, 1)\n",
    "    real_data = real_data.view(batch_size, 1).to(device)\n",
    "    fake_data = fake_data.view(batch_size, 1).to(device)\n",
    "\n",
    "    # Sample Epsilon from uniform distribution\n",
    "    eps = torch.rand(batch_size, 1).to(device)\n",
    "\n",
    "    # Interpolation between real data and fake data\n",
    "    interpolation = eps * real_data + (1 - eps) * fake_data\n",
    "\n",
    "    # Get logits for interpolated data\n",
    "    interp_logits = netD(interpolation)\n",
    "\n",
    "    # Compute Gradients\n",
    "    gradients = autograd.grad(\n",
    "        outputs=interp_logits,\n",
    "        inputs=interpolation,\n",
    "        grad_outputs=torch.ones_like(interp_logits).to(device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "\n",
    "    # Compute and return Gradient Norm\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    grad_norm = gradients.norm(2, dim=1)\n",
    "    return torch.mean((grad_norm - 1) ** 2)\n",
    "\n",
    "\n",
    "#for data in data_loader:\n",
    "    # data is a batch of size (batch_size, 2)\n",
    "    # Do whatever you want with it\n",
    "    #print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_Gaussian_dimension, number_of_grain_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_Gaussian_dimension, 75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(75, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, number_of_grain_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, number_of_grain_features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(number_of_grain_features, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 75),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(75, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def generator_loss(discriminator, fake_grains):\n",
    "    \"\"\"Loss computed to train the GAN generator.\n",
    "\n",
    "    Args:\n",
    "      discriminator: The discriminator whose forward function takes inputs of shape (batch_size, 2)\n",
    "         and produces outputs of shape (batch_size, 1).\n",
    "      fake_grains of shape (batch_size, 2): Fake grains produces by the generator.\n",
    "\n",
    "    Returns:\n",
    "      loss: The mean of the binary cross-entropy losses computed for all the samples in the batch.\n",
    "\n",
    "    Notes:\n",
    "    - Make sure that you process on the device given by `fake_grains.device`.\n",
    "    - Use values of global variables `real_label`, `fake_label` to produce the right targets.\n",
    "    \"\"\"\n",
    "    # Ensure that the target tensor has the same device as the input fake_grains\n",
    "    target = torch.full((fake_grains.size(0),), real_label, device=fake_grains.device)\n",
    "    \n",
    "    # Forward pass through the discriminator with the fake grains\n",
    "    output = discriminator(fake_grains).view(-1)\n",
    "    \n",
    "    # Compute the binary cross-entropy loss between the output and the target\n",
    "    loss = F.binary_cross_entropy(output, target, reduction='mean')\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def discriminator_loss(discriminator, real_grains, fake_grains):\n",
    "    \"\"\"Loss computed to train the GAN discriminator.\n",
    "\n",
    "    Args:\n",
    "      discriminator: The discriminator.\n",
    "      real_grains of shape (batch_size, 2): Real grains.\n",
    "      fake_grains of shape (batch_size, 2): Fake grains produces by the generator.\n",
    "\n",
    "    Returns:\n",
    "      d_loss_real: The mean of the binary cross-entropy losses computed on the real_grains.\n",
    "      D_real: Mean output of the discriminator for real_grains. This is useful for tracking convergence.\n",
    "      d_loss_fake: The mean of the binary cross-entropy losses computed on the fake_grains.\n",
    "      D_fake: Mean output of the discriminator for fake_grains. This is useful for tracking convergence.\n",
    "\n",
    "    Notes:\n",
    "    - Make sure that you process on the device given by `fake_grains.device`.\n",
    "    - Use values of global variables `real_label`, `fake_label` to produce the right targets.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()\n",
    "    # Ensure that the target tensors have the same device as the input grains\n",
    "    device = fake_grains.device  # Get the device of the input tensors\n",
    "    \n",
    "    # Transfer input tensors to the same device as the discriminator\n",
    "    real_grains = real_grains.to(device)\n",
    "    fake_grains = fake_grains.to(device)\n",
    "    \n",
    "    # Create the target labels for real and fake grains\n",
    "    real_target = torch.full((real_grains.size(0),), real_label, device=device)\n",
    "    fake_target = torch.full((fake_grains.size(0),), fake_label, device=device)\n",
    "    \n",
    "    # Compute the discriminator outputs for real and fake grains\n",
    "    real_output = discriminator(real_grains).view(-1)\n",
    "    fake_output = discriminator(fake_grains).view(-1)\n",
    "    \n",
    "    # Compute the binary cross-entropy losses\n",
    "    d_loss_real = nn.BCELoss()(real_output, real_target)\n",
    "    d_loss_fake = nn.BCELoss()(fake_output, fake_target)\n",
    "    \n",
    "    # Compute the mean discriminator outputs for real and fake grains\n",
    "    D_real = real_output.mean().item()\n",
    "    D_fake = fake_output.mean().item()\n",
    "    \n",
    "    return d_loss_real, D_real, d_loss_fake, D_fake\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing shapes and loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6315, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test generator loss\n",
    "def test_generator_loss():\n",
    "    # Create a generator\n",
    "    netG = Generator(latent_Gaussian_dimension=2, number_of_grain_features=2)\n",
    "    netD = Discriminator(number_of_grain_features=2)\n",
    "    # Create fake grains\n",
    "    noise = torch.randn(100, 2)\n",
    "    fake_grains = netG(noise)\n",
    "    # Compute the generator loss\n",
    "    loss = generator_loss(netD, fake_grains)\n",
    "    print(loss)\n",
    "\n",
    "test_generator_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7006, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "0.4963092803955078\n",
      "tensor(0.6873, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "0.4970460534095764\n"
     ]
    }
   ],
   "source": [
    "# Test discriminator loss\n",
    "def test_discriminator_loss():\n",
    "    # Create a generator\n",
    "    netG = Generator(latent_Gaussian_dimension=2, number_of_grain_features=2)\n",
    "    netD = Discriminator(number_of_grain_features=2)\n",
    "    # Create real and fake grains\n",
    "    real_grains = torch.randn(100, 2)\n",
    "    noise = torch.randn(100, 2)\n",
    "    fake_grains = netG(noise)\n",
    "    # Compute the discriminator loss\n",
    "    d_loss_real, D_real, d_loss_fake, D_fake = discriminator_loss(netD, real_grains, fake_grains)\n",
    "    print(d_loss_real)\n",
    "    print(D_real)\n",
    "    print(d_loss_fake)\n",
    "    print(D_fake)\n",
    "\n",
    "test_discriminator_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Generator_shapes():\n",
    "    netG = Generator(100, 2)\n",
    "\n",
    "    batch_size = 32\n",
    "    noise = torch.randn(batch_size, 100)\n",
    "    out = netG(noise)\n",
    "\n",
    "    assert out.shape == torch.Size([batch_size, 2]), f\"Bad shape of out: out.shape={out.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Generator_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Discriminator_shapes():\n",
    "    netD = Discriminator(2)\n",
    "\n",
    "    batch_size = 32\n",
    "    noise = torch.randn(batch_size, 2)\n",
    "    out = netD(noise)\n",
    "\n",
    "    assert out.shape == torch.Size([batch_size, 1]), f\"Bad shape of out: out.shape={out.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Discriminator_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Discriminator Loss: D_real=0.9774, D_fake=0.3200\n",
      "Generator Loss: 1.1528\n",
      "Epoch 5\n",
      "Discriminator Loss: D_real=0.5929, D_fake=0.4193\n",
      "Generator Loss: 1.1813\n",
      "Epoch 10\n",
      "Discriminator Loss: D_real=0.4580, D_fake=0.4856\n",
      "Generator Loss: 0.6522\n",
      "Epoch 15\n",
      "Discriminator Loss: D_real=0.5141, D_fake=0.5179\n",
      "Generator Loss: 0.7206\n",
      "Epoch 20\n",
      "Discriminator Loss: D_real=0.4825, D_fake=0.4830\n",
      "Generator Loss: 0.6711\n",
      "Epoch 25\n",
      "Discriminator Loss: D_real=0.4386, D_fake=0.4546\n",
      "Generator Loss: 0.4839\n",
      "Epoch 30\n",
      "Discriminator Loss: D_real=0.4433, D_fake=0.4602\n",
      "Generator Loss: 0.5284\n",
      "Epoch 35\n",
      "Discriminator Loss: D_real=0.4736, D_fake=0.4693\n",
      "Generator Loss: 0.7054\n",
      "Epoch 40\n",
      "Discriminator Loss: D_real=0.4651, D_fake=0.4721\n",
      "Generator Loss: 0.5923\n",
      "Epoch 45\n",
      "Discriminator Loss: D_real=0.5229, D_fake=0.5295\n",
      "Generator Loss: 0.7736\n",
      "Epoch 50\n",
      "Discriminator Loss: D_real=0.4972, D_fake=0.4950\n",
      "Generator Loss: 0.6971\n",
      "Epoch 55\n",
      "Discriminator Loss: D_real=0.4906, D_fake=0.4839\n",
      "Generator Loss: 0.8150\n",
      "Epoch 60\n",
      "Discriminator Loss: D_real=0.4894, D_fake=0.4891\n",
      "Generator Loss: 0.7281\n",
      "Epoch 65\n",
      "Discriminator Loss: D_real=0.4813, D_fake=0.4838\n",
      "Generator Loss: 0.6487\n",
      "Epoch 70\n",
      "Discriminator Loss: D_real=0.4824, D_fake=0.4634\n",
      "Generator Loss: 0.9200\n",
      "Epoch 75\n",
      "Discriminator Loss: D_real=0.5140, D_fake=0.5052\n",
      "Generator Loss: 0.7640\n",
      "Epoch 80\n",
      "Discriminator Loss: D_real=0.5068, D_fake=0.4971\n",
      "Generator Loss: 0.7971\n",
      "Epoch 85\n",
      "Discriminator Loss: D_real=0.5200, D_fake=0.5189\n",
      "Generator Loss: 0.7319\n",
      "Epoch 90\n",
      "Discriminator Loss: D_real=0.5094, D_fake=0.5015\n",
      "Generator Loss: 0.6944\n",
      "Epoch 95\n",
      "Discriminator Loss: D_real=0.5024, D_fake=0.4966\n",
      "Generator Loss: 0.6181\n",
      "Epoch 100\n",
      "Discriminator Loss: D_real=0.5000, D_fake=0.4987\n",
      "Generator Loss: 0.6841\n",
      "Epoch 105\n",
      "Discriminator Loss: D_real=0.5023, D_fake=0.4939\n",
      "Generator Loss: 0.7165\n",
      "Epoch 110\n",
      "Discriminator Loss: D_real=0.4745, D_fake=0.4687\n",
      "Generator Loss: 0.7568\n",
      "Epoch 115\n",
      "Discriminator Loss: D_real=0.4893, D_fake=0.4788\n",
      "Generator Loss: 0.8595\n",
      "Epoch 120\n",
      "Discriminator Loss: D_real=0.4769, D_fake=0.4779\n",
      "Generator Loss: 0.7241\n",
      "Epoch 125\n",
      "Discriminator Loss: D_real=0.4981, D_fake=0.4946\n",
      "Generator Loss: 0.7441\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39m# Generate fake grains\u001b[39;00m\n\u001b[0;32m     64\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(batch_size, latent_Gaussian_dimension, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m---> 65\u001b[0m fake_grains \u001b[39m=\u001b[39m generator(noise)\n\u001b[0;32m     67\u001b[0m \u001b[39m# Compute generator loss\u001b[39;00m\n\u001b[0;32m     68\u001b[0m g_loss \u001b[39m=\u001b[39m generator_loss(discriminator, fake_grains)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[38], line 20\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "latent_Gaussian_dimension = 100  # Dimension of the input noise vector\n",
    "number_of_grain_features = 2  # Dimension of the real data\n",
    "#real_data_dim = 30218  # Dimension of the real data\n",
    "number_of_reduced_grains = 1000  # Dimension of the generated data\n",
    "\n",
    "generator = Generator(latent_Gaussian_dimension, number_of_grain_features).to(device)\n",
    "discriminator = Discriminator(number_of_grain_features).to(device)\n",
    "\n",
    "# Define optimizer. The paper mentions using RMSProp\n",
    "#generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "generator_optimizer = optim.RMSprop(generator.parameters(), lr=0.00005)\n",
    "discriminator_optimizer = optim.RMSprop(discriminator.parameters(), lr=0.00005)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(grainsData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs + 1):\n",
    "    for batch_index, real_grains in enumerate(dataloader):\n",
    "        real_grains = real_grains.to(torch.float32)\n",
    "        #print(real_grains.dtype)\n",
    "        #print(real_grains.shape)\n",
    "        # real_grains is of shape (batch_size, number_of_grain_features)\n",
    "\n",
    "        ################################\n",
    "        # Update Discriminator network #\n",
    "        ################################\n",
    "\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        noise = torch.randn(batch_size, latent_Gaussian_dimension, device=device, dtype=torch.float32)\n",
    "        fake_grains = generator(noise)\n",
    "\n",
    "        #print(fake_grains)\n",
    "        #print(fake_grains.dtype)\n",
    "        #print(fake_grains.shape)\n",
    "\n",
    "        #time.sleep(60)\n",
    "        # Compute discriminator loss\n",
    "        d_loss_real, D_real, d_loss_fake, D_fake = discriminator_loss(discriminator, real_grains, fake_grains)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        # Update discriminator\n",
    "        d_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        \n",
    "        ############################\n",
    "        # Update Generator network\n",
    "        ###########################\n",
    "        generator_optimizer.zero_grad()\n",
    "        \n",
    "        # Generate fake grains\n",
    "        noise = torch.randn(batch_size, latent_Gaussian_dimension, device=device)\n",
    "        fake_grains = generator(noise)\n",
    "        \n",
    "        # Compute generator loss\n",
    "        g_loss = generator_loss(discriminator, fake_grains)\n",
    "        \n",
    "        # Update generator\n",
    "        g_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "            \n",
    "    ############################\n",
    "    # Print training progress\n",
    "    ###########################\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"Discriminator Loss: D_real={D_real:.4f}, D_fake={D_fake:.4f}\")\n",
    "        print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Generate fake grains and save into csv file\n",
    "            generate_times = 20\n",
    "            # for each epoch, we wil generate 20 different groups of fake grains as generator is random\n",
    "            outputPath = f\"downsampled_grains/epoch_{epoch}\"\n",
    "            if os.path.exists(outputPath):\n",
    "                # remove the directory if it exists and create new\n",
    "                shutil.rmtree(outputPath)\n",
    "                os.makedirs(outputPath)\n",
    "            else:\n",
    "                os.makedirs(outputPath)\n",
    "            \n",
    "            for i in range(generate_times):  \n",
    "                outputPathIndex = f\"downsampled_grains/epoch_{epoch}\" \n",
    "                noise = torch.randn(number_of_reduced_grains, latent_Gaussian_dimension, device=device)\n",
    "                fake_grains = generator(noise)\n",
    "                fake_grains = fake_grains.cpu().numpy()\n",
    "                #print(fake_grains.shape)\n",
    "                # Save as csv file, use pandas\n",
    "                columns = [\"grain_R\", \"grain_asp\"]\n",
    "                df = pd.DataFrame(fake_grains, columns=columns)\n",
    "                df.to_csv(f\"{outputPathIndex}/grains_{i+1}.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the implementation of the WGAN\n",
    "two feedforward neural networks were applied: One as the discriminator and one as the\n",
    "generator. For the activation functions a ReLu function was used for all but the Output layer of the\n",
    "discriminator, which uses a Sigmoid function, since the result of this layer has to be a probability and\n",
    "Sigmoid functions give results between 0 and 1. For all of these implementations, the Pytorch\n",
    "library was used\n",
    "\n",
    "The most important ones are the width and depth of the network, as explained above. These two hyperparameters are key for the training\n",
    "process, since they determine the number of neurons and the depth of the network. For each neuron,\n",
    "a weight exists as was explained above. By adapting the weights of the neurons for each processed\n",
    "batch the network actually learns the target distribution. Therefore, the number of neurons in a\n",
    "layer, as well as the amount of layers contributes immensely to the quality of the trained NN. Thus,\n",
    "width and depth were iteratively fitted in this study. This was done by training multiple NN with\n",
    "varying hyperparameter sets (especially for width and depth), which were changed iteratively in a\n",
    "looped approach. Therefore the parameter sets to be tested were defined before the training and then\n",
    "subsequently tested.\n",
    "\n",
    "the training data set is split into many, randomly-generated sub sets. The algorithm processes each\n",
    "mini batch separately and compares the cost function in relation to the current mini batch data.\n",
    "Subsequently, the network parameters are updated accordingly. This is done iterating over all mini\n",
    "batches, until the whole data set has been evaluated. This complete process is called an epoch. For the\n",
    "training in this study, a batch size of 64 was used, \n",
    "\n",
    "However, the learning rate was automatically adapted, since an optimisation\n",
    "algorithm called RMSProp was used for the back propagation. This optimisation approach requires a low\n",
    "learning rate which is inherent to WGAN networks. Thus the influence of these parameters is negligible.\n",
    "Two important parameters are the clipping value and the dropout. Both variables represent values that\n",
    "are important in the NN to avoid overfitting and underfitting\n",
    "\n",
    "When the framework is defined, the training of the NN begins. Since the NN changes slightly after\n",
    "every epoch, it is important to create intermediate snapshots of the NN, as well as the output data in the\n",
    "form of a CSV file after a defined number of epochs, 200 in the case of this study. Additionally, only a\n",
    "defined number of epochs should be trained, since MLA are prone to overfitting in higher epochs.\n",
    "\n",
    "After the training of all the different NN for multiple parameter sets is complete, the best fit needs\n",
    "to be found. To do so, a script was written, which creates a KDE of every microstructural parameter\n",
    "taken into account for the MLA training. Additionally, the KDE of every taken snapshot are created of\n",
    "the same parameters, since this is the output of the generator network. Subsequently the fit between\n",
    "the input KDE and synthetic data KDE are evaluated. This is done by investigating three types of\n",
    "deviation between the KDE of the real and synthetic data: The mean deviation, maximum deviation,\n",
    "and mean value of both mean and maximum deviation. All three are returned and can be checked by\n",
    "the user. Ideally, a NN snapshot can be chosen that shows the smallest deviation for all three values in\n",
    "comparison to the rest of the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
