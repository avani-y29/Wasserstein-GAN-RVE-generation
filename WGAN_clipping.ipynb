{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries regarding torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde, lognorm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.8169461\n",
      "3.989422804\n",
      "6.209501941\n",
      "1.0\n",
      "30218\n",
      "30218\n",
      "(30218, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading from RSA_input.csv\n",
    "df = pd.read_csv('RSA_input.csv') # add csv file to the correct location\n",
    "grain_R = df[\"grain_R\"]\n",
    "grain_asp = df[\"grain_asp\"]\n",
    "print(grain_R.max())\n",
    "print(grain_R.min())\n",
    "print(grain_asp.max())\n",
    "print(grain_asp.min())\n",
    "print(len(grain_R)) # length 30218\n",
    "print(len(grain_asp)) # length 30218\n",
    "\n",
    "# Combined into numpy shape (30218, 2)\n",
    "grainsData = np.column_stack((grain_R, grain_asp))\n",
    "print(grainsData.shape)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = -1.\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_Gaussian_dimension, number_of_grain_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # change width and depth of the network here\n",
    "            nn.Linear(latent_Gaussian_dimension, 120),\n",
    "            #nn.BatchNorm1d(120),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Linear(120, 80),\n",
    "            #nn.BatchNorm1d(80),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Linear(80, 40),\n",
    "            #nn.BatchNorm1d(40),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Linear(40, number_of_grain_features),\n",
    "            #nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, number_of_grain_features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # change width and depth of the network here\n",
    "            nn.Linear(number_of_grain_features, 40),\n",
    "            #nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(40, 80),\n",
    "            #nn.BatchNorm1d(80),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(80, 120),\n",
    "            #nn.BatchNorm1d(120),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(120, 160),\n",
    "            #nn.BatchNorm1d(160),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(160, 1)\n",
    "\n",
    "            # The output of discriminator is no longer a probability, \n",
    "            # we do not apply sigmoid at the output of discriminator.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-5\n",
    "batch_size = 3000\n",
    "generator_iterations = 2\n",
    "critic_iterations = 3\n",
    "weight_clipping_limit = 1\n",
    "num_epochs = 1000\n",
    "printAndSaveEvery_N_Epoch = 1\n",
    "# Initialize generator and discriminator\n",
    "latent_Gaussian_dimension = 160  # Dimension of the input noise vector\n",
    "number_of_grain_features = 2  # Dimension of the real data\n",
    "#real_data_dim = 30218  # Dimension of the real data\n",
    "number_of_reduced_grains = 1000  # Dimension of the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Generator Loss: -0.0252\n",
      "Discriminator Loss: d_loss_real=3.0107, d_loss_fake=-3.0337\n",
      "Wasserstein Distance: 3.0337\n",
      "Epoch 1\n",
      "Generator Loss: -0.0669\n",
      "Discriminator Loss: d_loss_real=10.1945, d_loss_fake=-10.2579\n",
      "Wasserstein Distance: 10.2579\n",
      "Epoch 2\n",
      "Generator Loss: 0.0055\n",
      "Discriminator Loss: d_loss_real=22.5139, d_loss_fake=-22.5040\n",
      "Wasserstein Distance: 22.5040\n",
      "Epoch 3\n",
      "Generator Loss: 0.6205\n",
      "Discriminator Loss: d_loss_real=41.6614, d_loss_fake=-41.0451\n",
      "Wasserstein Distance: 41.0451\n",
      "Epoch 4\n",
      "Generator Loss: 3.2250\n",
      "Discriminator Loss: d_loss_real=68.6034, d_loss_fake=-65.4109\n",
      "Wasserstein Distance: 65.4109\n",
      "Epoch 5\n",
      "Generator Loss: 9.8685\n",
      "Discriminator Loss: d_loss_real=105.3289, d_loss_fake=-95.5773\n",
      "Wasserstein Distance: 95.5773\n",
      "Epoch 6\n",
      "Generator Loss: 22.9706\n",
      "Discriminator Loss: d_loss_real=154.4157, d_loss_fake=-131.6459\n",
      "Wasserstein Distance: 131.6459\n",
      "Epoch 7\n",
      "Generator Loss: 46.0919\n",
      "Discriminator Loss: d_loss_real=215.5263, d_loss_fake=-169.8832\n",
      "Wasserstein Distance: 169.8832\n",
      "Epoch 8\n",
      "Generator Loss: 83.2912\n",
      "Discriminator Loss: d_loss_real=288.3941, d_loss_fake=-205.6806\n",
      "Wasserstein Distance: 205.6806\n",
      "Epoch 9\n",
      "Generator Loss: 140.3439\n",
      "Discriminator Loss: d_loss_real=377.8781, d_loss_fake=-238.5930\n",
      "Wasserstein Distance: 238.5930\n",
      "Epoch 10\n",
      "Generator Loss: 222.6407\n",
      "Discriminator Loss: d_loss_real=477.3355, d_loss_fake=-256.1250\n",
      "Wasserstein Distance: 256.1250\n",
      "Epoch 11\n",
      "Generator Loss: 332.4608\n",
      "Discriminator Loss: d_loss_real=587.4382, d_loss_fake=-256.5816\n",
      "Wasserstein Distance: 256.5816\n",
      "Epoch 12\n",
      "Generator Loss: 472.9239\n",
      "Discriminator Loss: d_loss_real=705.0906, d_loss_fake=-234.8550\n",
      "Wasserstein Distance: 234.8550\n",
      "Epoch 13\n",
      "Generator Loss: 639.1916\n",
      "Discriminator Loss: d_loss_real=820.2940, d_loss_fake=-183.5205\n",
      "Wasserstein Distance: 183.5205\n",
      "Epoch 14\n",
      "Generator Loss: 811.3921\n",
      "Discriminator Loss: d_loss_real=916.0152, d_loss_fake=-105.8179\n",
      "Wasserstein Distance: 105.8179\n",
      "Epoch 15\n",
      "Generator Loss: 937.7430\n",
      "Discriminator Loss: d_loss_real=941.9285, d_loss_fake=-2.8965\n",
      "Wasserstein Distance: 2.8965\n",
      "Epoch 16\n",
      "Generator Loss: 863.9373\n",
      "Discriminator Loss: d_loss_real=797.2844, d_loss_fake=73.9482\n",
      "Wasserstein Distance: -73.9482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m d_loss_real \u001b[39m=\u001b[39m discriminator(real_grains)\n\u001b[0;32m     51\u001b[0m d_loss_real \u001b[39m=\u001b[39m d_loss_real\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m d_loss_real\u001b[39m.\u001b[39;49mbackward(minus_one)\n\u001b[0;32m     54\u001b[0m \u001b[39m# Train with fake grains\u001b[39;00m\n\u001b[0;32m     55\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(batch_size, latent_Gaussian_dimension, device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize gen and discriminator_loss\n",
    "generator = Generator(latent_Gaussian_dimension, number_of_grain_features).to(device)\n",
    "discriminator = Discriminator(number_of_grain_features).to(device)\n",
    "\n",
    "#initialize optimizer\n",
    "generator_optimizer = optim.RMSprop(generator.parameters(), lr=learning_rate)\n",
    "discriminator_optimizer = optim.RMSprop(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(grainsData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialize tensorboard plotting\n",
    "\n",
    "# train gen & critic in loop\n",
    "for epoch in range(0, num_epochs + 2):\n",
    "    g_loss_list = []\n",
    "    #d_loss_list = []\n",
    "    d_loss_real_list = []\n",
    "    d_loss_fake_list = []\n",
    "    Wasserstein_distance_list = []\n",
    "    \n",
    "    for batch_index, real_grains in enumerate(dataloader):\n",
    "        real_grains = real_grains.to(torch.float32)\n",
    "        # Requires grad, Generator requires_grad = False\n",
    "\n",
    "        one = torch.FloatTensor([1])\n",
    "        minus_one = one * -1\n",
    "            \n",
    "        for gen_iter in range(generator_iterations): \n",
    "            for param in discriminator.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Train discriminator = max E(discriminator(real) - E(dicriminator(fake)))\n",
    "            #d_loss_iter = []\n",
    "            d_loss_real_iter = []\n",
    "            d_loss_fake_iter = []\n",
    "            Wasserstein_distance_iter = []\n",
    "            for critic_iter in range(critic_iterations):\n",
    "                discriminator.zero_grad()\n",
    "\n",
    "                # Clamp parameters to a range [-c, c], c=self.weight_cliping_limit\n",
    "                #for param in discriminator.parameters():\n",
    "                #    param.data.clamp_(-weight_clipping_limit, weight_clipping_limit)\n",
    "\n",
    "                # Train discriminator\n",
    "                # WGAN - Training discriminator more iterations than generator\n",
    "                # Train with real grains\n",
    "\n",
    "                d_loss_real = discriminator(real_grains)\n",
    "                d_loss_real = d_loss_real.mean().view(1)\n",
    "                d_loss_real.backward(minus_one)\n",
    "\n",
    "                # Train with fake grains\n",
    "                noise = torch.randn(batch_size, latent_Gaussian_dimension, device=device, dtype=torch.float32)\n",
    "                fake_grains = generator(noise)\n",
    "\n",
    "                d_loss_fake = discriminator(fake_grains)\n",
    "                d_loss_fake = d_loss_fake.mean().view(1)\n",
    "                d_loss_fake.backward(one)\n",
    "\n",
    "                d_loss = d_loss_fake - d_loss_real\n",
    "                Wasserstein_distance = d_loss_real - d_loss_fake\n",
    "                discriminator_optimizer.step()\n",
    "\n",
    "                #d_loss_iter.append(d_loss.item())\n",
    "                d_loss_real_iter.append(d_loss_real.item())\n",
    "                d_loss_fake_iter.append(d_loss.item())\n",
    "                Wasserstein_distance_iter.append(Wasserstein_distance.item())\n",
    "            \n",
    "            #d_loss_list.append(np.mean(d_loss_iter))\n",
    "            d_loss_real_list.append(np.mean(d_loss_real_iter))\n",
    "            d_loss_fake_list.append(np.mean(d_loss_fake_iter))\n",
    "            Wasserstein_distance_list.append(np.mean(Wasserstein_distance_iter))\n",
    "            # print(f'Discriminator iteration: {critic_iter}/{critic_iterations}, loss_fake: {d_loss_fake.data}, loss_real: {d_loss_real.data}')\n",
    "\n",
    "            # Generator update\n",
    "            for param in discriminator.parameters():\n",
    "                param.requires_grad = False  # to avoid computation\n",
    "\n",
    "            generator.zero_grad()\n",
    "\n",
    "            # Train generator\n",
    "            # Compute loss with fake grains\n",
    "            noise = torch.randn(batch_size, latent_Gaussian_dimension, device=device, dtype=torch.float32)\n",
    "            fake_grains = generator(noise)\n",
    "            g_loss = discriminator(fake_grains)\n",
    "            g_loss = g_loss.mean().view(1)\n",
    "            g_loss.backward(minus_one)\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            # print(f'Generator iteration: {gen_iter}/{generator_iterations}, g_loss: {g_loss.data}')\n",
    "    \n",
    "    ############################\n",
    "    # Print training progress\n",
    "    ###########################\n",
    "    \n",
    "    if epoch % printAndSaveEvery_N_Epoch == 0:\n",
    "        g_loss_value = np.mean(g_loss_list)\n",
    "        d_loss_real_value = np.mean(d_loss_real_list)\n",
    "        d_loss_fake_value = np.mean(d_loss_fake_list)\n",
    "        #d_loss = np.mean(d_loss_list)\n",
    "        Wasserstein_distance_value = np.mean(Wasserstein_distance_list)\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"Generator Loss: {g_loss_value:.4f}\")\n",
    "        print(f\"Discriminator Loss: d_loss_real={d_loss_real_value:.4f}, d_loss_fake={d_loss_fake_value:.4f}\")\n",
    "        print(f\"Wasserstein Distance: {Wasserstein_distance_value:.4f}\")\n",
    "        \n",
    "    \n",
    "    if epoch % printAndSaveEvery_N_Epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            # Generate fake grains and save into csv file\n",
    "            generate_times = 20\n",
    "            # for each epoch, we wil generate 20 different groups of fake grains as generator is random\n",
    "            outputPath = f\"downsampled_grains/epoch_{epoch}\"\n",
    "            if os.path.exists(outputPath):\n",
    "                # remove the directory if it exists and create new\n",
    "                shutil.rmtree(outputPath)\n",
    "                os.makedirs(outputPath)\n",
    "            else:\n",
    "                os.makedirs(outputPath)\n",
    "            \n",
    "            for i in range(generate_times):  \n",
    "                outputPathIndex = f\"downsampled_grains/epoch_{epoch}\" \n",
    "                noise = torch.randn(number_of_reduced_grains, latent_Gaussian_dimension, device=device)\n",
    "                fake_grains = generator(noise)\n",
    "                fake_grains = fake_grains.cpu().numpy()\n",
    "                #print(fake_grains.shape)\n",
    "                # Save as csv file, use pandas\n",
    "                columns = [\"grain_R\", \"grain_asp\"]\n",
    "                df = pd.DataFrame(fake_grains, columns=columns)\n",
    "                df.to_csv(f\"{outputPathIndex}/grains_{i+1}.csv\", index=False)\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
