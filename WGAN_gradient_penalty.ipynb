{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries regarding torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde, lognorm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.8169461\n",
      "3.989422804\n",
      "6.209501941\n",
      "1.0\n",
      "30218\n",
      "30218\n",
      "(30218, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reading from RSA_input.csv\n",
    "df = pd.read_csv('RSA_input.csv') # add csv file to the correct location\n",
    "grain_R = df[\"grain_R\"]\n",
    "grain_asp = df[\"grain_asp\"]\n",
    "print(grain_R.max())\n",
    "print(grain_R.min())\n",
    "print(grain_asp.max())\n",
    "print(grain_asp.min())\n",
    "print(len(grain_R)) # length 30218\n",
    "print(len(grain_asp)) # length 30218\n",
    "\n",
    "# Combined into numpy shape (30218, 2)\n",
    "grainsData = np.column_stack((grain_R, grain_asp))\n",
    "print(grainsData.shape)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_Gaussian_dimension, number_of_grain_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # change width and depth of the network here\n",
    "            nn.Linear(latent_Gaussian_dimension, 120),\n",
    "            #nn.BatchNorm1d(120),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Linear(120, 80),\n",
    "            #nn.BatchNorm1d(80),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Linear(80, 40),\n",
    "            #nn.BatchNorm1d(40),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Linear(40, number_of_grain_features),\n",
    "            #nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, number_of_grain_features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # change width and depth of the network here\n",
    "            nn.Linear(number_of_grain_features, 40),\n",
    "            #nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(40, 80),\n",
    "            #nn.BatchNorm1d(80),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(80, 120),\n",
    "            #nn.BatchNorm1d(120),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(120, 160),\n",
    "            #nn.BatchNorm1d(160),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(160, 1)\n",
    "\n",
    "            # The output of discriminator is no longer a probability, \n",
    "            # we do not apply sigmoid at the output of discriminator.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient penalty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 5e-5\n",
    "batch_size = 3000\n",
    "generator_iterations = 2\n",
    "critic_iterations = 3\n",
    "weight_clipping_limit = 1\n",
    "num_epochs = 1000\n",
    "printAndSaveEvery_N_Epoch = 1\n",
    "# Initialize generator and discriminator\n",
    "latent_Gaussian_dimension = 160  # Dimension of the input noise vector\n",
    "number_of_grain_features = 2  # Dimension of the real data\n",
    "#real_data_dim = 30218  # Dimension of the real data\n",
    "number_of_reduced_grains = 1000  # Dimension of the generated data\n",
    "\n",
    "\n",
    "# WGAN values from paper\n",
    "learning_rate = 1e-4\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_penalty(discriminator, real_grains, fake_grains):\n",
    "    lambda_term = 10\n",
    "    batch_size_local = real_grains.shape[0]\n",
    "    #print(real_grains.shape)\n",
    "    eta = torch.FloatTensor(batch_size_local, number_of_grain_features).uniform_(0,1)\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    cuda_index = 0\n",
    "    if cuda:\n",
    "        eta = eta.cuda(cuda_index)\n",
    "    else:\n",
    "        eta = eta\n",
    "    #print(eta.shape)\n",
    "    #print(real_grains.shape)\n",
    "    #print(fake_grains.shape)\n",
    "    interpolated = eta * real_grains + ((1 - eta) * fake_grains)\n",
    "\n",
    "    if cuda:\n",
    "        interpolated = interpolated.cuda(cuda_index)\n",
    "    else:\n",
    "        interpolated = interpolated\n",
    "\n",
    "    # define it to calculate gradient\n",
    "    interpolated = Variable(interpolated, requires_grad=True)\n",
    "\n",
    "    # calculate probability of interpolated examples\n",
    "    prob_interpolated = discriminator(interpolated)\n",
    "\n",
    "    # calculate gradients of probabilities with respect to examples\n",
    "    gradients = autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                            grad_outputs=torch.ones(\n",
    "                                prob_interpolated.size()).cuda(cuda_index) if cuda else torch.ones(\n",
    "                                prob_interpolated.size()),\n",
    "                            create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    grad_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_term\n",
    "    return grad_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Generator Loss: 0.7351\n",
      "Discriminator Loss: d_loss_real=8.7299, d_loss_fake=-6.0885\n",
      "Wasserstein Distance: 7.9552\n",
      "Epoch 1\n",
      "Generator Loss: 1.6471\n",
      "Discriminator Loss: d_loss_real=3.5755, d_loss_fake=-1.6609\n",
      "Wasserstein Distance: 1.8870\n",
      "Epoch 2\n",
      "Generator Loss: 10.5391\n",
      "Discriminator Loss: d_loss_real=10.7402, d_loss_fake=0.1060\n",
      "Wasserstein Distance: 0.1235\n",
      "Epoch 3\n",
      "Generator Loss: 3.1694\n",
      "Discriminator Loss: d_loss_real=3.4151, d_loss_fake=0.2843\n",
      "Wasserstein Distance: 0.1171\n",
      "Epoch 4\n",
      "Generator Loss: 0.4053\n",
      "Discriminator Loss: d_loss_real=0.6584, d_loss_fake=0.4657\n",
      "Wasserstein Distance: 0.2279\n",
      "Epoch 5\n",
      "Generator Loss: -0.7384\n",
      "Discriminator Loss: d_loss_real=-0.5437, d_loss_fake=0.1471\n",
      "Wasserstein Distance: 0.0842\n",
      "Epoch 6\n",
      "Generator Loss: -0.6873\n",
      "Discriminator Loss: d_loss_real=-0.3893, d_loss_fake=0.2965\n",
      "Wasserstein Distance: 0.2017\n",
      "Epoch 7\n",
      "Generator Loss: -3.6274\n",
      "Discriminator Loss: d_loss_real=-3.4531, d_loss_fake=0.0030\n",
      "Wasserstein Distance: 0.1293\n",
      "Epoch 8\n",
      "Generator Loss: 0.3031\n",
      "Discriminator Loss: d_loss_real=0.5079, d_loss_fake=0.2817\n",
      "Wasserstein Distance: 0.1354\n",
      "Epoch 9\n",
      "Generator Loss: 0.7679\n",
      "Discriminator Loss: d_loss_real=0.9004, d_loss_fake=0.1348\n",
      "Wasserstein Distance: 0.0272\n",
      "Epoch 10\n",
      "Generator Loss: 2.5649\n",
      "Discriminator Loss: d_loss_real=2.7551, d_loss_fake=0.0996\n",
      "Wasserstein Distance: 0.1176\n",
      "Epoch 11\n",
      "Generator Loss: -1.5417\n",
      "Discriminator Loss: d_loss_real=-1.4053, d_loss_fake=0.2769\n",
      "Wasserstein Distance: 0.1113\n",
      "Epoch 12\n",
      "Generator Loss: 1.1973\n",
      "Discriminator Loss: d_loss_real=1.5625, d_loss_fake=0.2920\n",
      "Wasserstein Distance: 0.2975\n",
      "Epoch 13\n",
      "Generator Loss: 1.9606\n",
      "Discriminator Loss: d_loss_real=2.0659, d_loss_fake=0.2632\n",
      "Wasserstein Distance: 0.0991\n",
      "Epoch 14\n",
      "Generator Loss: 0.0258\n",
      "Discriminator Loss: d_loss_real=0.3115, d_loss_fake=0.1286\n",
      "Wasserstein Distance: 0.2010\n",
      "Epoch 15\n",
      "Generator Loss: -1.6431\n",
      "Discriminator Loss: d_loss_real=-1.4625, d_loss_fake=0.2961\n",
      "Wasserstein Distance: 0.1122\n",
      "Epoch 16\n",
      "Generator Loss: -11.0500\n",
      "Discriminator Loss: d_loss_real=-10.8400, d_loss_fake=0.1786\n",
      "Wasserstein Distance: 0.1315\n",
      "Epoch 17\n",
      "Generator Loss: -8.7745\n",
      "Discriminator Loss: d_loss_real=-8.6219, d_loss_fake=0.1659\n",
      "Wasserstein Distance: 0.0694\n",
      "Epoch 18\n",
      "Generator Loss: -2.1076\n",
      "Discriminator Loss: d_loss_real=-1.9678, d_loss_fake=0.1128\n",
      "Wasserstein Distance: 0.1368\n",
      "Epoch 19\n",
      "Generator Loss: -4.8965\n",
      "Discriminator Loss: d_loss_real=-4.7748, d_loss_fake=0.1210\n",
      "Wasserstein Distance: 0.0925\n",
      "Epoch 20\n",
      "Generator Loss: -0.0123\n",
      "Discriminator Loss: d_loss_real=0.2334, d_loss_fake=0.1258\n",
      "Wasserstein Distance: 0.1497\n",
      "Epoch 21\n",
      "Generator Loss: -10.0937\n",
      "Discriminator Loss: d_loss_real=-9.9167, d_loss_fake=0.1238\n",
      "Wasserstein Distance: 0.1446\n",
      "Epoch 22\n",
      "Generator Loss: -4.8104\n",
      "Discriminator Loss: d_loss_real=-4.5763, d_loss_fake=0.1310\n",
      "Wasserstein Distance: 0.1102\n",
      "Epoch 23\n",
      "Generator Loss: 7.8404\n",
      "Discriminator Loss: d_loss_real=8.0285, d_loss_fake=0.0162\n",
      "Wasserstein Distance: 0.1491\n",
      "Epoch 24\n",
      "Generator Loss: -4.7508\n",
      "Discriminator Loss: d_loss_real=-4.5565, d_loss_fake=0.0042\n",
      "Wasserstein Distance: 0.1647\n",
      "Epoch 25\n",
      "Generator Loss: -1.6621\n",
      "Discriminator Loss: d_loss_real=-1.5555, d_loss_fake=0.0984\n",
      "Wasserstein Distance: 0.0758\n",
      "Epoch 26\n",
      "Generator Loss: 15.4123\n",
      "Discriminator Loss: d_loss_real=15.5410, d_loss_fake=0.1514\n",
      "Wasserstein Distance: 0.0553\n",
      "Epoch 27\n",
      "Generator Loss: 17.6184\n",
      "Discriminator Loss: d_loss_real=17.8101, d_loss_fake=0.0247\n",
      "Wasserstein Distance: 0.1441\n",
      "Epoch 28\n",
      "Generator Loss: -9.4925\n",
      "Discriminator Loss: d_loss_real=-9.4200, d_loss_fake=0.1888\n",
      "Wasserstein Distance: -0.0140\n",
      "Epoch 29\n",
      "Generator Loss: -13.7335\n",
      "Discriminator Loss: d_loss_real=-13.5562, d_loss_fake=0.1025\n",
      "Wasserstein Distance: 0.1187\n",
      "Epoch 30\n",
      "Generator Loss: -11.9404\n",
      "Discriminator Loss: d_loss_real=-11.7356, d_loss_fake=-0.1044\n",
      "Wasserstein Distance: 0.1940\n",
      "Epoch 31\n",
      "Generator Loss: -11.2941\n",
      "Discriminator Loss: d_loss_real=-11.1313, d_loss_fake=0.0516\n",
      "Wasserstein Distance: 0.1267\n",
      "Epoch 32\n",
      "Generator Loss: 10.1922\n",
      "Discriminator Loss: d_loss_real=10.3371, d_loss_fake=0.0843\n",
      "Wasserstein Distance: 0.1267\n",
      "Epoch 33\n",
      "Generator Loss: -24.3117\n",
      "Discriminator Loss: d_loss_real=-24.1152, d_loss_fake=0.0591\n",
      "Wasserstein Distance: 0.0802\n",
      "Epoch 34\n",
      "Generator Loss: -40.7064\n",
      "Discriminator Loss: d_loss_real=-40.4749, d_loss_fake=-0.0154\n",
      "Wasserstein Distance: 0.1420\n",
      "Epoch 35\n",
      "Generator Loss: -19.0686\n",
      "Discriminator Loss: d_loss_real=-19.0611, d_loss_fake=0.2296\n",
      "Wasserstein Distance: -0.0351\n",
      "Epoch 36\n",
      "Generator Loss: -26.3118\n",
      "Discriminator Loss: d_loss_real=-26.0149, d_loss_fake=-0.1218\n",
      "Wasserstein Distance: 0.2869\n",
      "Epoch 37\n",
      "Generator Loss: -23.8680\n",
      "Discriminator Loss: d_loss_real=-23.7070, d_loss_fake=0.0705\n",
      "Wasserstein Distance: 0.0785\n",
      "Epoch 38\n",
      "Generator Loss: -30.5682\n",
      "Discriminator Loss: d_loss_real=-30.3415, d_loss_fake=0.0310\n",
      "Wasserstein Distance: 0.1441\n",
      "Epoch 39\n",
      "Generator Loss: -39.7527\n",
      "Discriminator Loss: d_loss_real=-39.5651, d_loss_fake=0.0273\n",
      "Wasserstein Distance: 0.0886\n",
      "Epoch 40\n",
      "Generator Loss: -25.6707\n",
      "Discriminator Loss: d_loss_real=-25.5007, d_loss_fake=0.0438\n",
      "Wasserstein Distance: 0.1190\n",
      "Epoch 41\n",
      "Generator Loss: -21.1467\n",
      "Discriminator Loss: d_loss_real=-20.9821, d_loss_fake=0.0486\n",
      "Wasserstein Distance: 0.0761\n",
      "Epoch 42\n",
      "Generator Loss: -5.3102\n",
      "Discriminator Loss: d_loss_real=-5.1067, d_loss_fake=-0.0303\n",
      "Wasserstein Distance: 0.1549\n",
      "Epoch 43\n",
      "Generator Loss: -14.2463\n",
      "Discriminator Loss: d_loss_real=-14.0272, d_loss_fake=0.0239\n",
      "Wasserstein Distance: 0.1112\n",
      "Epoch 44\n",
      "Generator Loss: -16.9103\n",
      "Discriminator Loss: d_loss_real=-16.8230, d_loss_fake=0.1948\n",
      "Wasserstein Distance: -0.0244\n",
      "Epoch 45\n",
      "Generator Loss: 5.7781\n",
      "Discriminator Loss: d_loss_real=5.7161, d_loss_fake=0.2506\n",
      "Wasserstein Distance: -0.0608\n",
      "Epoch 46\n",
      "Generator Loss: 9.0833\n",
      "Discriminator Loss: d_loss_real=9.1644, d_loss_fake=0.2169\n",
      "Wasserstein Distance: -0.0306\n",
      "Epoch 47\n",
      "Generator Loss: 12.2792\n",
      "Discriminator Loss: d_loss_real=12.5792, d_loss_fake=0.0055\n",
      "Wasserstein Distance: 0.2209\n",
      "Epoch 48\n",
      "Generator Loss: 3.5510\n",
      "Discriminator Loss: d_loss_real=3.7027, d_loss_fake=0.1072\n",
      "Wasserstein Distance: 0.0925\n",
      "Epoch 49\n",
      "Generator Loss: -4.3009\n",
      "Discriminator Loss: d_loss_real=-4.1759, d_loss_fake=0.0707\n",
      "Wasserstein Distance: 0.0673\n",
      "Epoch 50\n",
      "Generator Loss: 7.6658\n",
      "Discriminator Loss: d_loss_real=7.9182, d_loss_fake=-0.0129\n",
      "Wasserstein Distance: 0.1593\n",
      "Epoch 51\n",
      "Generator Loss: -23.8984\n",
      "Discriminator Loss: d_loss_real=-23.7922, d_loss_fake=0.0656\n",
      "Wasserstein Distance: 0.0500\n",
      "Epoch 52\n",
      "Generator Loss: 0.8532\n",
      "Discriminator Loss: d_loss_real=1.0547, d_loss_fake=0.0759\n",
      "Wasserstein Distance: 0.1170\n",
      "Epoch 53\n",
      "Generator Loss: -4.7833\n",
      "Discriminator Loss: d_loss_real=-4.6283, d_loss_fake=0.1045\n",
      "Wasserstein Distance: 0.0948\n",
      "Epoch 54\n",
      "Generator Loss: 15.5791\n",
      "Discriminator Loss: d_loss_real=15.7085, d_loss_fake=0.0776\n",
      "Wasserstein Distance: 0.0912\n",
      "Epoch 55\n",
      "Generator Loss: 24.0547\n",
      "Discriminator Loss: d_loss_real=24.2238, d_loss_fake=0.0489\n",
      "Wasserstein Distance: 0.1005\n",
      "Epoch 56\n",
      "Generator Loss: 18.1740\n",
      "Discriminator Loss: d_loss_real=18.3659, d_loss_fake=-0.0000\n",
      "Wasserstein Distance: 0.1363\n",
      "Epoch 57\n",
      "Generator Loss: 25.8923\n",
      "Discriminator Loss: d_loss_real=26.0889, d_loss_fake=0.0497\n",
      "Wasserstein Distance: 0.1401\n",
      "Epoch 58\n",
      "Generator Loss: 9.1650\n",
      "Discriminator Loss: d_loss_real=9.2868, d_loss_fake=0.0334\n",
      "Wasserstein Distance: 0.1216\n",
      "Epoch 59\n",
      "Generator Loss: 1.6996\n",
      "Discriminator Loss: d_loss_real=1.9999, d_loss_fake=0.0336\n",
      "Wasserstein Distance: 0.1918\n",
      "Epoch 60\n",
      "Generator Loss: 13.5910\n",
      "Discriminator Loss: d_loss_real=13.7180, d_loss_fake=0.0847\n",
      "Wasserstein Distance: 0.0854\n",
      "Epoch 61\n",
      "Generator Loss: 21.1606\n",
      "Discriminator Loss: d_loss_real=21.3081, d_loss_fake=0.0528\n",
      "Wasserstein Distance: 0.1152\n",
      "Epoch 62\n",
      "Generator Loss: 18.3000\n",
      "Discriminator Loss: d_loss_real=18.4341, d_loss_fake=0.0487\n",
      "Wasserstein Distance: 0.0747\n",
      "Epoch 63\n",
      "Generator Loss: 13.3218\n",
      "Discriminator Loss: d_loss_real=13.6378, d_loss_fake=-0.0273\n",
      "Wasserstein Distance: 0.2461\n",
      "Epoch 64\n",
      "Generator Loss: 10.6499\n",
      "Discriminator Loss: d_loss_real=10.7549, d_loss_fake=0.1705\n",
      "Wasserstein Distance: 0.0645\n",
      "Epoch 65\n",
      "Generator Loss: 27.7393\n",
      "Discriminator Loss: d_loss_real=28.0068, d_loss_fake=0.0330\n",
      "Wasserstein Distance: 0.1518\n",
      "Epoch 66\n",
      "Generator Loss: 31.9112\n",
      "Discriminator Loss: d_loss_real=32.0156, d_loss_fake=0.1688\n",
      "Wasserstein Distance: 0.0485\n",
      "Epoch 67\n",
      "Generator Loss: 56.6026\n",
      "Discriminator Loss: d_loss_real=56.7142, d_loss_fake=0.0439\n",
      "Wasserstein Distance: 0.0988\n",
      "Epoch 68\n",
      "Generator Loss: 20.1862\n",
      "Discriminator Loss: d_loss_real=20.2651, d_loss_fake=0.1941\n",
      "Wasserstein Distance: -0.0007\n",
      "Epoch 69\n",
      "Generator Loss: 12.9524\n",
      "Discriminator Loss: d_loss_real=13.0999, d_loss_fake=0.0555\n",
      "Wasserstein Distance: 0.0392\n",
      "Epoch 70\n",
      "Generator Loss: 16.0820\n",
      "Discriminator Loss: d_loss_real=16.0800, d_loss_fake=0.1532\n",
      "Wasserstein Distance: -0.0527\n",
      "Epoch 71\n",
      "Generator Loss: 17.7988\n",
      "Discriminator Loss: d_loss_real=17.7762, d_loss_fake=0.0886\n",
      "Wasserstein Distance: 0.0052\n",
      "Epoch 72\n",
      "Generator Loss: 18.1099\n",
      "Discriminator Loss: d_loss_real=18.2845, d_loss_fake=0.0739\n",
      "Wasserstein Distance: 0.0795\n",
      "Epoch 73\n",
      "Generator Loss: 19.7029\n",
      "Discriminator Loss: d_loss_real=19.8588, d_loss_fake=0.1050\n",
      "Wasserstein Distance: 0.0884\n",
      "Epoch 74\n",
      "Generator Loss: 29.1817\n",
      "Discriminator Loss: d_loss_real=29.4669, d_loss_fake=-0.0482\n",
      "Wasserstein Distance: 0.2310\n",
      "Epoch 75\n",
      "Generator Loss: 57.3095\n",
      "Discriminator Loss: d_loss_real=57.4324, d_loss_fake=0.0227\n",
      "Wasserstein Distance: 0.1236\n",
      "Epoch 76\n",
      "Generator Loss: 58.3557\n",
      "Discriminator Loss: d_loss_real=58.5696, d_loss_fake=0.0290\n",
      "Wasserstein Distance: 0.0781\n",
      "Epoch 77\n",
      "Generator Loss: 33.0149\n",
      "Discriminator Loss: d_loss_real=33.1822, d_loss_fake=0.0582\n",
      "Wasserstein Distance: 0.0994\n",
      "Epoch 78\n",
      "Generator Loss: 5.7607\n",
      "Discriminator Loss: d_loss_real=5.9364, d_loss_fake=-0.0060\n",
      "Wasserstein Distance: 0.1430\n",
      "Epoch 79\n",
      "Generator Loss: -1.0221\n",
      "Discriminator Loss: d_loss_real=-0.8364, d_loss_fake=0.0237\n",
      "Wasserstein Distance: 0.1205\n",
      "Epoch 80\n",
      "Generator Loss: 1.9926\n",
      "Discriminator Loss: d_loss_real=1.9801, d_loss_fake=0.1101\n",
      "Wasserstein Distance: -0.0157\n",
      "Epoch 81\n",
      "Generator Loss: 19.8660\n",
      "Discriminator Loss: d_loss_real=19.9411, d_loss_fake=0.0797\n",
      "Wasserstein Distance: 0.0360\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m         g_loss \u001b[39m=\u001b[39m g_loss\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m)\n\u001b[0;32m     91\u001b[0m         g_loss\u001b[39m.\u001b[39mbackward(minus_one)\n\u001b[1;32m---> 92\u001b[0m         generator_optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     94\u001b[0m         g_loss_list\u001b[39m.\u001b[39mappend(g_loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     95\u001b[0m         \u001b[39m# print(f'Generator iteration: {gen_iter}/{generator_iterations}, g_loss: {g_loss.data}')\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[0;32m     97\u001b[0m \u001b[39m############################\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m# Print training progress\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m###########################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:337\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    335\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m--> 337\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39;49mis_complex(param):\n\u001b[0;32m    338\u001b[0m     grad \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(grad)\n\u001b[0;32m    339\u001b[0m     exp_avg \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(exp_avg)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize gen and discriminator_loss\n",
    "generator = Generator(latent_Gaussian_dimension, number_of_grain_features).to(device)\n",
    "discriminator = Discriminator(number_of_grain_features).to(device)\n",
    "\n",
    "#initialize optimizer\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(grainsData, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialize tensorboard plotting\n",
    "\n",
    "# train gen & critic in loop\n",
    "for epoch in range(0, num_epochs + 2):\n",
    "    g_loss_list = []\n",
    "    #d_loss_list = []\n",
    "    d_loss_real_list = []\n",
    "    d_loss_fake_list = []\n",
    "    Wasserstein_distance_list = []\n",
    "    \n",
    "    for batch_index, real_grains in enumerate(dataloader):\n",
    "        batch_size_local = real_grains.shape[0]\n",
    "        real_grains = real_grains.to(torch.float32)\n",
    "        # Requires grad, Generator requires_grad = False\n",
    "\n",
    "        one = torch.FloatTensor([1])\n",
    "        minus_one = one * -1\n",
    "            \n",
    "        for gen_iter in range(generator_iterations): \n",
    "            for param in discriminator.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Train discriminator = max E(discriminator(real) - E(dicriminator(fake)))\n",
    "            #d_loss_iter = []\n",
    "            d_loss_real_iter = []\n",
    "            d_loss_fake_iter = []\n",
    "            Wasserstein_distance_iter = []\n",
    "            for critic_iter in range(critic_iterations):\n",
    "                discriminator.zero_grad()\n",
    "\n",
    "                # Train discriminator\n",
    "                # WGAN - Training discriminator more iterations than generator\n",
    "                # Train with real grains\n",
    "\n",
    "                d_loss_real = discriminator(real_grains)\n",
    "                d_loss_real = d_loss_real.mean().view(1)\n",
    "                d_loss_real.backward(minus_one)\n",
    "\n",
    "                # Train with fake grains\n",
    "                noise = torch.randn(batch_size_local, latent_Gaussian_dimension, device=device, dtype=torch.float32)\n",
    "                fake_grains = generator(noise)\n",
    "\n",
    "                d_loss_fake = discriminator(fake_grains)\n",
    "                d_loss_fake = d_loss_fake.mean().view(1)\n",
    "                d_loss_fake.backward(one)\n",
    "\n",
    "                # Train with gradient penalty\n",
    "                gradient_penalty = calculate_gradient_penalty(discriminator, real_grains.data, fake_grains.data)\n",
    "                gradient_penalty.backward()\n",
    "\n",
    "                d_loss = d_loss_fake - d_loss_real + gradient_penalty\n",
    "\n",
    "                Wasserstein_distance = d_loss_real - d_loss_fake\n",
    "                discriminator_optimizer.step()\n",
    "\n",
    "                #d_loss_iter.append(d_loss.item())\n",
    "                d_loss_real_iter.append(d_loss_real.item())\n",
    "                d_loss_fake_iter.append(d_loss.item())\n",
    "                Wasserstein_distance_iter.append(Wasserstein_distance.item())\n",
    "            \n",
    "            #d_loss_list.append(np.mean(d_loss_iter))\n",
    "            d_loss_real_list.append(np.mean(d_loss_real_iter))\n",
    "            d_loss_fake_list.append(np.mean(d_loss_fake_iter))\n",
    "            Wasserstein_distance_list.append(np.mean(Wasserstein_distance_iter))\n",
    "            # print(f'Discriminator iteration: {critic_iter}/{critic_iterations}, loss_fake: {d_loss_fake.data}, loss_real: {d_loss_real.data}')\n",
    "\n",
    "            # Generator update\n",
    "            for param in discriminator.parameters():\n",
    "                param.requires_grad = False  # to avoid computation\n",
    "\n",
    "            generator.zero_grad()\n",
    "\n",
    "            # Train generator\n",
    "            # Compute loss with fake grains\n",
    "            noise = torch.randn(batch_size_local, latent_Gaussian_dimension, device=device, dtype=torch.float32)\n",
    "            fake_grains = generator(noise)\n",
    "            g_loss = discriminator(fake_grains)\n",
    "            g_loss = g_loss.mean().view(1)\n",
    "            g_loss.backward(minus_one)\n",
    "            generator_optimizer.step()\n",
    "\n",
    "            g_loss_list.append(g_loss.item())\n",
    "            # print(f'Generator iteration: {gen_iter}/{generator_iterations}, g_loss: {g_loss.data}')\n",
    "    \n",
    "    ############################\n",
    "    # Print training progress\n",
    "    ###########################\n",
    "    \n",
    "    if epoch % printAndSaveEvery_N_Epoch == 0:\n",
    "        g_loss_value = np.mean(g_loss_list)\n",
    "        d_loss_real_value = np.mean(d_loss_real_list)\n",
    "        d_loss_fake_value = np.mean(d_loss_fake_list)\n",
    "        #d_loss = np.mean(d_loss_list)\n",
    "        Wasserstein_distance_value = np.mean(Wasserstein_distance_list)\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"Generator Loss: {g_loss_value:.4f}\")\n",
    "        print(f\"Discriminator Loss: d_loss_real={d_loss_real_value:.4f}, d_loss_fake={d_loss_fake_value:.4f}\")\n",
    "        print(f\"Wasserstein Distance: {Wasserstein_distance_value:.4f}\")\n",
    "        \n",
    "    \n",
    "    if epoch % printAndSaveEvery_N_Epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            # Generate fake grains and save into csv file\n",
    "            generate_times = 20\n",
    "            # for each epoch, we wil generate 20 different groups of fake grains as generator is random\n",
    "            outputPath = f\"downsampled_grains/epoch_{epoch}\"\n",
    "            if os.path.exists(outputPath):\n",
    "                # remove the directory if it exists and create new\n",
    "                shutil.rmtree(outputPath)\n",
    "                os.makedirs(outputPath)\n",
    "            else:\n",
    "                os.makedirs(outputPath)\n",
    "            \n",
    "            for i in range(generate_times):  \n",
    "                outputPathIndex = f\"downsampled_grains/epoch_{epoch}\" \n",
    "                noise = torch.randn(number_of_reduced_grains, latent_Gaussian_dimension, device=device)\n",
    "                fake_grains = generator(noise)\n",
    "                fake_grains = fake_grains.cpu().numpy()\n",
    "                #print(fake_grains.shape)\n",
    "                # Save as csv file, use pandas\n",
    "                columns = [\"grain_R\", \"grain_asp\"]\n",
    "                df = pd.DataFrame(fake_grains, columns=columns)\n",
    "                df.to_csv(f\"{outputPathIndex}/grains_{i+1}.csv\", index=False)\n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
